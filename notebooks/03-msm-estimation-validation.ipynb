{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSM estimation and validation\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "In this notebook, we will cover how to estimate a Markov state model (MSM) and do model validation;\n",
    "we also show how to save and restore model and estimator objects.\n",
    "For this notebook, you need to know how to do data loading/visualization as well as dimension reduction.\n",
    "\n",
    "\n",
    "**Remember**:\n",
    "- to run the currently highlighted cell, hold <kbd>&#x21E7; Shift</kbd> and press <kbd>&#x23ce; Enter</kbd>;\n",
    "- to get help for a specific function, place the cursor within the function's brackets, hold <kbd>&#x21E7; Shift</kbd>, and press <kbd>&#x21E5; Tab</kbd>;\n",
    "- you can find the full documentation at [PyEMMA.org](http://www.pyemma.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import mdshare\n",
    "import pyemma\n",
    "import deeptime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MD data and repeating the clustering step\n",
    "\n",
    "Let's load alanine dipeptide backbone torsions and discretise with 200 $k$-means centers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb = mdshare.fetch('alanine-dipeptide-nowater.pdb', working_directory='data')\n",
    "files = mdshare.fetch('alanine-dipeptide-*-250ns-nowater.xtc', working_directory='data')\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "feat.add_backbone_torsions(periodic=False)\n",
    "\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "data_concatenated = np.concatenate(data)\n",
    "\n",
    "cluster = dt.clustering.Kmeans(200, max_iter=50).fit(data_concatenated[::10]).fetch_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and plot the free energy along with the cluster centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "pyemma.plots.plot_free_energy(*data_concatenated.T, ax=ax, legacy=False)\n",
    "ax.scatter(*cluster.cluster_centers.T, s=15, c='k')\n",
    "ax.set_xlabel('$\\Phi$ / rad') \n",
    "ax.set_ylabel('$\\Psi$ / rad')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implied time scales and lag time selection\n",
    "\n",
    "The first step after obtaining the discretized dynamics is finding a suitable lag time.\n",
    "The systematic approach is to estimate MSMs at various lag times and observe how the implied timescales (ITSs) of these models behave.\n",
    "In particular, we are looking for lag time ranges in which the implied timescales are constant\n",
    "To this aim, PyEMMA provides the `its()` function which we use to track the first four (`nits=4`) implied timescales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrajs = [cluster.transform(traj) for traj in data]\n",
    "\n",
    "lags = [1, 2, 5, 10, 20, 50]\n",
    "timescales = []\n",
    "for lag in tqdm(lags):\n",
    "    counts_estimator = dt.markov.TransitionCountEstimator(lag, \"sliding\")\n",
    "    counts = counts_estimator.fit(dtrajs).fetch_model()\n",
    "    counts = counts.submodel_largest()\n",
    "    \n",
    "    msm_estimator = dt.markov.msm.MaximumLikelihoodMSM()\n",
    "    msm = msm_estimator.fit(counts).fetch_model()\n",
    "    timescales.append(msm.timescales(k=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray*}\n",
    "T(n \\tau) & = & (T(\\tau))^n\\\\[0.75em]\n",
    "\\lambda(n \\tau) & = & (\\lambda(\\tau))^n\\\\[0.75em]\n",
    "\\mathrm{ITS}(n \\tau) & = & - \\frac{n \\tau}{\\ln \\lambda(n \\tau)} = - \\frac{n \\tau}{\\ln (\\lambda(\\tau))^n} = - \\frac{\\tau}{\\ln \\lambda(\\tau)} = \\mathrm{ITS}(\\tau)\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "We can pass the returned `its` object to the `pyemma.plots.plot_implied_timescales()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_implied_timescales((lags, timescales), units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot tells us that there are three resolved processes (blue, red, green) which are largely invariant to the MSM lag time.\n",
    "The fourth ITS (cyan) is smaller than the lag time (black line, grey-shaded area);\n",
    "it corresponds to a process which is faster than the lag time and, thus, is not resolved.\n",
    "Since the implied timescales are, like the corresponding eigenvalues, sorted in decreasing order,\n",
    "we know that all other remaining processes must be even faster.\n",
    "\n",
    "## Error bars for the timescales\n",
    "\n",
    "To compute error bars, pass the `errors=bayes` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1, 2, 5, 10, 20, 50]\n",
    "timescales = []\n",
    "timescales_samples = []\n",
    "\n",
    "for lag in tqdm(lags):\n",
    "    counts_estimator = dt.markov.TransitionCountEstimator(lag, \"effective\")\n",
    "    counts = counts_estimator.fit(dtrajs).fetch_model()\n",
    "    counts = counts.submodel_largest()\n",
    "    \n",
    "    bmsm_estimator = dt.markov.msm.BayesianMSM()\n",
    "    bmsm = bmsm_estimator.fit(counts).fetch_model()\n",
    "    timescales.append(bmsm.prior.timescales(k=4))\n",
    "    timescales_samples.append(bmsm.evaluate_samples('timescales', k=4).T)\n",
    "\n",
    "pyemma.plots.plot_implied_timescales((lags, timescales, timescales_samples), units='ps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of the discretization on the implied timescales\n",
    "\n",
    "Let's look at the discretisation's influence on the ITSs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def its(k, data):\n",
    "    cluster = dt.clustering.Kmeans(k, max_iter=50).fit(data[::10]).fetch_model()\n",
    "    dtrajs = cluster.transform(data)\n",
    "    \n",
    "    lags = [1, 2, 5, 10, 20, 50]\n",
    "    models = []\n",
    "    msm_estimator = dt.markov.msm.MaximumLikelihoodMSM()\n",
    "    estimator = dt.markov.msm.BayesianMSM()\n",
    "    for lag in tqdm(lags):\n",
    "        msm_estimator.lagtime = lag\n",
    "        msm = msm_estimator.fit(dtrajs, count_mode=\"effective\")\n",
    "        bmsm = estimator.fit(msm).fetch_model()\n",
    "        models.append(bmsm)\n",
    "    \n",
    "    ts = [x.prior.timescales(4) for x in models]\n",
    "    ts_s = [x.evaluate_samples('timescales', k=4).T for x in models]\n",
    "    return cluster, (lags, ts, ts_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_20, its_20 = its(20, data_concatenated)\n",
    "cluster_50, its_50 = its(50, data_concatenated)\n",
    "cluster_100, its_100 = its(100, data_concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "pyemma.plots.plot_free_energy(*data_concatenated.T, ax=axes[0, 0], cbar=False)\n",
    "axes[0, 0].scatter(*cluster_20.cluster_centers.T, s=15, c='k')\n",
    "pyemma.plots.plot_implied_timescales(its_20, ax=axes[1, 0], units='ps')\n",
    "\n",
    "pyemma.plots.plot_free_energy(*data_concatenated.T, ax=axes[0, 1], cbar=False)\n",
    "axes[0, 1].scatter(*cluster_50.cluster_centers.T, s=15, c='k')\n",
    "pyemma.plots.plot_implied_timescales(its_50, ax=axes[1, 1], units='ps')\n",
    "\n",
    "pyemma.plots.plot_free_energy(*data_concatenated.T, ax=axes[0, 2], cbar=False)\n",
    "axes[0, 2].scatter(*cluster_100.cluster_centers.T, s=15, c='k')\n",
    "pyemma.plots.plot_implied_timescales(its_100, ax=axes[1, 2], units='ps')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the maximum likelihood Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_estimator = dt.markov.TransitionCountEstimator(lagtime=10, count_mode='sliding')\n",
    "counts = counts_estimator.fit(dtrajs).fetch_model().submodel_largest()\n",
    "\n",
    "msm_estimator = dt.markov.msm.MaximumLikelihoodMSM()\n",
    "msm = msm_estimator.fit(counts).fetch_model()\n",
    "\n",
    "print(f'fraction of states used = {msm.state_fraction}')\n",
    "print(f'fraction of counts used = {msm.count_fraction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msm.timescales(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state space can be restricted to largest connected set (`submodel_largest()`) or any other selection of states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.submodel([0, 1, 3, 7])\n",
    "print(f\"States: {counts.states}, state symbols: {counts.state_symbols}\")\n",
    "msm =dt.markov.msm.MaximumLikelihoodMSM().fit(counts).fetch_model()\n",
    "\n",
    "print(f'fraction of states used = {msm.state_fraction}')\n",
    "print(f'fraction of counts used = {msm.count_fraction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And restricted even further, always based on the _states_ of the current count model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.submodel([0, 3])\n",
    "print(f\"States: {counts.states}, state symbols: {counts.state_symbols}\")\n",
    "msm = dt.markov.msm.MaximumLikelihoodMSM().fit(counts).fetch_model()\n",
    "\n",
    "print(f'fraction of states used = {msm.state_fraction}')\n",
    "print(f'fraction of counts used = {msm.count_fraction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Bayesian Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_estimator = dt.markov.TransitionCountEstimator(lagtime=10, count_mode='effective')\n",
    "counts = count_estimator.fit(dtrajs).fetch_model()\n",
    "counts = counts.submodel_largest()\n",
    "bayesian_msm_estimator = dt.markov.msm.BayesianMSM()\n",
    "bayesian_msm = bayesian_msm_estimator.fit(counts).fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = bayesian_msm.gather_stats('timescales', k=3)\n",
    "stats.L, stats.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Chapman-Kolmogorov test\n",
    "\n",
    "To see whether our model satisfies Markovianity, we perform (and visualize) a Chapman-Kolmogorow (CK) test.\n",
    "Since we aim at modeling the dynamics between metastable states rather than between microstates, this will be conducted in the space of metastable states.\n",
    "The latter are identified automatically using PCCA++ (which is explained later).\n",
    "We usually choose the number of metastable states according to the implied timescales plot by identifying a gap between the ITS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_estimator = dt.markov.msm.MaximumLikelihoodMSM(lagtime=10)\n",
    "test_estimator.fit(dtrajs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_validator = test_estimator.chapman_kolmogorov_validator(n_metastable_sets=4, mlags=10)\n",
    "ck_test = ck_validator.fit(dtrajs).fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(ck_test, units='ps');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_estimator = dt.markov.TransitionCountEstimator(lagtime=10, count_mode='effective')\n",
    "counts = count_estimator.fit(dtrajs).fetch_model().submodel_largest()\n",
    "bmsm_estimator = dt.markov.msm.BayesianMSM()\n",
    "bmsm_estimator.fit(counts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_val = bmsm_estimator.chapman_kolmogorov_validator(4, mlags=10)\n",
    "cktest = ck_val.fit(dtrajs, progress=tqdm).fetch_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemma.plots.plot_cktest(cktest, units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting and restoring estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cluster_50.pkl', 'wb') as f:\n",
    "    pickle.dump(cluster_50, f)\n",
    "    \n",
    "with open('cluster_50.pkl', 'rb') as f:\n",
    "    cluster_50_restored = pickle.load(f)\n",
    "    \n",
    "print(cluster_50_restored.n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('msm.pkl', 'wb') as f:\n",
    "    pickle.dump(msm, f)\n",
    "with open('msm.pkl', 'rb') as f:\n",
    "    msm_restored = pickle.load(f)\n",
    "    \n",
    "print(f\"Timescales {msm.timescales()}, restored {msm_restored.timescales()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "\n",
    "#### Exercise 1\n",
    "\n",
    "Load the heavy atom distances into memory, perform PCA and TICA (`lag=3`) with `dim=2`,\n",
    "then discretize with $100$ $k$-means centers and a stride of $10$. Compare the two discretizations be generating implied timescale plots for both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feat =  #FIXME\n",
    "feat. #FIXME\n",
    "data =  #FIXME\n",
    "\n",
    "pca_estimator = PCA(2)\n",
    "pca = #FIXME\n",
    "tica = #FIXME\n",
    "\n",
    "pca_concatenated = np.concatenate(pca.get_output())\n",
    "tica_concatenated = #FIXME\n",
    "\n",
    "cls_pca_estimator = dt.clustering.Kmeans(100, max_iter=50)\n",
    "cls_pca = #FIXME\n",
    "cls_tica = #FIXME\n",
    "\n",
    "dtrajs_pca = [cls_pca.transform(pca.transform(traj)) for traj in data]\n",
    "dtrajs_tica = # FIXME\n",
    "\n",
    "def its(dtrajs, lags):\n",
    "    models = []\n",
    "    estimator = dt.markov.msm.BayesianMSM()\n",
    "    for lag in tqdm(lags):\n",
    "        counts_estimator = dt.markov.TransitionCountEstimator(lag, \"effective\")\n",
    "        counts = # FIXME\n",
    "        bayesian_msm = # FIXME\n",
    "        models.append(bmsm)\n",
    "    \n",
    "    timescales = # FIXME\n",
    "    timescales_samples = # FIXME\n",
    "    return lags, timescales, timescales_samples\n",
    "\n",
    "    \n",
    "its_pca = its(dtrajs_pca, lags=[1, 2, 5, 10, 20, 50])\n",
    "its_tica = #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "###### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "feat = pyemma.coordinates.featurizer(pdb)\n",
    "pairs = feat.pairs(feat.select_Heavy())\n",
    "feat.add_distances(pairs, periodic=False)\n",
    "data = pyemma.coordinates.load(files, features=feat)\n",
    "\n",
    "pca = PCA(2)\n",
    "pca.fit(np.concatenate(data))\n",
    "\n",
    "tica_estimator = dt.decomposition.TICA(lagtime=3, dim=2)\n",
    "tica = tica_estimator.fit(data).fetch_model()\n",
    "\n",
    "pca_concatenated = np.concatenate([pca.transform(traj) for traj in data])\n",
    "tica_concatenated = np.concatenate([tica.transform(traj) for traj in data])\n",
    "\n",
    "cls_pca = dt.clustering.Kmeans(100, max_iter=50).fit(pca_concatenated).fetch_model()\n",
    "cls_tica = dt.clustering.Kmeans(100, max_iter=50).fit(tica_concatenated).fetch_model()\n",
    "\n",
    "def its(dtrajs, lags):\n",
    "    models = []\n",
    "    estimator = dt.markov.msm.BayesianMSM()\n",
    "    for lag in tqdm(lags):\n",
    "        counts_estimator = dt.markov.TransitionCountEstimator(lag, \"effective\")\n",
    "        counts = counts_estimator.fit(dtrajs).fetch_model().submodel_largest()\n",
    "        bmsm = estimator.fit(counts).fetch_model()\n",
    "        models.append(bmsm)\n",
    "    \n",
    "    timescales = [x.prior.timescales(4) for x in models]\n",
    "    timescales_samples = [x.evaluate_samples('timescales', k=4).T for x in models]\n",
    "    return lags, timescales, timescales_samples\n",
    "\n",
    "\n",
    "dtrajs_pca = [cls_pca.transform(pca.transform(traj)) for traj in data]\n",
    "dtrajs_tica = [cls_tica.transform(tica.transform(traj)) for traj in data]\n",
    "its_pca = its(dtrajs_pca, lags=[1, 2, 5, 10, 20, 50])\n",
    "its_tica = its(dtrajs_tica, lags=[1, 2, 5, 10, 20, 50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the ITS convergence for both projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "pyemma.plots.plot_feature_histograms(pca_concatenated, ax=axes[0, 0])\n",
    "pyemma.plots.plot_feature_histograms(tica_concatenated, ax=axes[1, 0])\n",
    "axes[0, 0].set_title('PCA')\n",
    "axes[1, 0].set_title('TICA')\n",
    "pyemma.plots.plot_density(*pca_concatenated.T, ax=axes[0, 1], cbar=False, alpha=0.1)\n",
    "axes[0, 1].scatter(*cls_pca.cluster_centers.T, s=15, c='C1')\n",
    "axes[0, 1].set_xlabel('PC 1')\n",
    "axes[0, 1].set_ylabel('PC 2')\n",
    "pyemma.plots.plot_density(*tica_concatenated.T, ax=axes[1, 1], cbar=False, alpha=0.1)\n",
    "axes[1, 1].scatter(*cls_tica.cluster_centers.T, s=15, c='C1')\n",
    "axes[1, 1].set_xlabel('IC 1')\n",
    "axes[1, 1].set_ylabel('IC 2')\n",
    "pyemma.plots.plot_implied_timescales(its_pca, ax=axes[0, 2], units='ps')\n",
    "pyemma.plots.plot_implied_timescales(its_tica, ax=axes[1, 2], units='ps')\n",
    "axes[0, 2].set_ylim(1, 2000)\n",
    "axes[1, 2].set_ylim(1, 2000)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the fact that PCA yields a projection with some defined basins,\n",
    "the ITS plot shows that only one \"slow\" process is resolved which is more than one order of magnitude too fast.\n",
    "\n",
    "TICA does find three slow processes which agree (in terms of the implied timescales) with the backbone torsions example above.\n",
    "\n",
    "We conclude that this PCA projection is not suitable to resolve the slow dynamics of alanine dipeptide and we will continue to estimate/validate the TICA-based projection.\n",
    "\n",
    "#### Exercise 2\n",
    "\n",
    "Estimate a Bayesian MSM at lag time $10$ ps and perform/show a CK test for four metastable states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "counts_estimator = dt.markov.TransitionCountEstimator(lagtime=10, count_mode=\"effective\")\n",
    "counts = counts_estimator.fit(dtrajs_tica).fetch_model().submodel_largest()\n",
    "bayesian_msm = # FIXME\n",
    "pyemma.plots. #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "###### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "counts_estimator = dt.markov.TransitionCountEstimator(lagtime=10, count_mode=\"effective\")\n",
    "counts = counts_estimator.fit(dtrajs_tica).fetch_model().submodel_largest()\n",
    "\n",
    "estimator = dt.markov.msm.BayesianMSM()\n",
    "estimator.fit(counts)\n",
    "validator = estimator.chapman_kolmogorov_validator(4, mlags=10)\n",
    "cktest = validator.fit(dtrajs_tica).fetch_model()\n",
    "pyemma.plots.plot_cktest(cktest, units='ps');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again see a good agreement between model prediction and re-estimation.\n",
    "\n",
    "## Wrapping up\n",
    "In this notebook, we have learned how to estimate a regular or Bayesian MSM from discretized molecular simulation data with `pyemma` and how to perform basic model validation.\n",
    "In detail, we have selected a suitable lag time by using\n",
    "- `pyemma.msm.its()` to obtain an implied timescale object and\n",
    "- `pyemma.plots.plot_implied_timescales()` to visualize the convergence of the implied timescales.\n",
    "\n",
    "We then have used\n",
    "- `pyemma.msm.estimate_markov_model()` to estimate a regular MSM,\n",
    "- `pyemma.msm.bayesian_markov_model()` to estimate a Bayesian MSM,\n",
    "- the `timescales()` method of an estimated MSM object to access its implied timescales,\n",
    "- the `cktest()` method of an estimated MSM object to perform a Chapman-Kolmogorow test, and\n",
    "- `pyemma.plots.plot_cktest()` to visualize the latter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
